#summary the Beta3 Server Installation
#labels Phase-Implementation,Phase-Deploy

<wiki:toc max_depth="3" />

= Preamble =
 * Most things are stable; you might have to dump your database between releases until we're out of beta.

 * These scripts do a lot of dns queries. You're gonna wanna install bind and configure it to use forwarders. Maybe something like google public dns to help mask your queries. Then point your resolv.conf to localhost

 * Once installed, monitor your dns traffic a bit, your server will be looking up some interesting information. Get a good handle on this and work with your security teams to whitelist this server.

= Before you Begin =
This framework was developed against debian (lenny, squeeze and ubuntu > 10.10 or so). This has *NOT* been tested against any other distro's yet (RHEL, BSD, etc). Purely due to lack of cycles. Look here for new doc to pop up when that's complete.

= System Requirements =
These requirements will handle everything on the same box pretty well with the default open source data-sets. The more (bigger) data-sets you add, the more ram / disk space you'll need. The more cores you add, the more threads that can "batch out" the feed parsers (thus, resulting in faster data consumption).

These specs will handle around 10k feeds at once with minimal impact on memory usage. Past that you'll need to start doubling your specs. Virtual machine technology is great for prototyping your implementation and will give you a good baseline of what you'll need for production.

 # an x86-64bit platform
 # at-least 8GB ram
 # at-least 4 cores
 # at-least 100GB of free (after OS install) disk space, which will last you about 6-9 months.

= Prerequisites =
== Required Services ==
 # Join the [http://groups.google.com/group/ci-framework mailing list] -- we like to archive the Q and A there. When you ask questions directly, I will usually Cc my responses.
 # Beta2_Removal (if you have beta2 installed this is *critical*)
 # DiskLayout
 # PostgresInstall
 # BindSetup

== System Deps ==
 # ServerInstall_Beta3_Generic
 # ServerInstall_Beta3_Lenny
 # ServerInstall_Beta3_Squeeze
 # ServerInstall_Beta3_Ubuntu10
 # [ServerInstall_Beta3_CentOS6]

= Install =
== Known Issues ==
 # If you've modified anything other than /opt/cif/etc/custom.cfg in /opt/cif/etc, *BACK UP YOUR etc/cif* directory. This install will overwrite all the main .cfg files
 # *IF YOU'RE USING A PREVIOUS VERSION OF CIF (aka: beta2), you'll need to drop your database, including any apikeys you've generated. We've added unix style "group" support to both the data-warehouse as well as the apikeys. Follow the Beta2_Removal guide*
 # libwww6 was released mid-stream on us; which drastically changes the way libwww works (LWP, etc). Before you begin, make sure you update your libwww-perl instance to v6 until we fix this in the Makefile.am:
{{{
$ sudo perl -MCPAN -e 'install LWP::Simple'
}}}
 # depending on what cpan mirror you use; Config::Simple 4.59 won't install (even though it's been out since like 2006), you might need to:
{{{
$ wget http://search.cpan.org/CPAN/authors/id/S/SH/SHERZODR/Config-Simple-4.59.tar.gz
$ tar -zxvf Config-Simple-4.59.tar.gz
$ cd Config-Simple-4.59 && perl Makefile.PL && sudo make install
}}}
 # we're using postgres as an embedded db, these current configs assume your postgres instance is locked down from the outside world. if someone gets shell, they've already got your data.

== Server ==
 # create the index / archive table spaces if you haven't via DiskLayout already (it's OK if you don't want to use LVM, these directories can exist on your root volume if you choose, but performance will increase if these are spread out across many disks):
{{{
$ sudo mkdir /mnt/archive
$ sudo mkdir /mnt/index
$ sudo chown postgres:postgres /mnt/index
$ sudo chown postgres:postgres /mnt/archive
$ sudo chmod 770 /mnt/index
$ sudo chmod 770 /mnt/archive
}}}
 # install the PerlClient (we use some functions in it for the server)
 # create your "cif" user/group (the configure script will default to this user "cif")
{{{
$ sudo adduser --disabled-password --gecos '' cif
}}}
 # if you're upgrading, make sure you've backed up /opt/cif/etc appropriately (if you've modified anything inside it)
 # install the core server interface
{{{
$ wget http://collective-intelligence-framework.googlecode.com/files/cif-0.01-b20111024_140745.tar.gz
$ tar -zxvf cif-0.01-b20111024_140745.tar.gz
$ cd cif-0.01-b20111024_140745
$ ./configure
$ make testdeps
$ make fixdeps
$ sudo make install
}}}
 # if this is a first-time install:
{{{
$ sudo make initdb
$ make tables
}}}
 # these types of messages are considered normal:
{{{
NOTICE:  table "domain" does not exist, skipping
}}}
 # if you're upgrading a previous install you'll need to restart apache2
{{{
$ sudo /etc/init.d/apache2 restart
}}}

== Configuration ==
 # log in as the cif user:
{{{
$ sudo su - cif
}}}
 # modify your local path, vi ~/.profile
{{{
if [ -d "/opt/cif/bin" ]; then
    PATH="/opt/cif/bin:$PATH"
fi
}}}
 # make a directory for your backups, where possible make this an NFS mount or SSHFS mount to another server
{{{
$ mkdir backups
}}}
 # generate your initial apikey to be used with your [ClientSetup client]
{{{
$ cif_apikeys -u myuser@mydomain.com -a -g everyone -G everyone
userid              key                                  description guid                                 default_guid access write revoked created                     
myuser@mydomain.com 4c3b44b2-8196-4af9-a77c-afb182793544             8c864306-d21a-37b1-8705-746a786719bf true         all                  2011-10-25 11:40:58.81532+00
}}}
 # check to make sure your 'guid' has _*8c864306-d21a-37b1-8705-746a786719bf*_ in it. If it doesn't you won't be able to see all the default, public data that's permissioned to the 'everyone' group in your system
 # to list all of your apikeys:
{{{
$ cif_apikeys -l
}}}
 # cif_apikeys -h will give you an example of how to use the tool
 # configure your ~/.cif for generating feeds
{{{
[cif_feeds]
maxrecords = 10000
severity_feeds = high,medium
confidence_feeds = 95,85
apikeys = role_everyone_feed
max_days = 2
disabled_feeds = hash,rir,asn,countrycode,malware
}}}
 # add this to /opt/cif/whitelist_infrastructure, make sure the file has the cif:cif permissions:
{{{
0.0.0.0/8
10.0.0.0/8
127.0.0.0/8
192.168.0.0/16
169.254.0.0/16
192.0.2.0/24
224.0.0.0/4
240.0.0.0/5
248.0.0.0/5
}}}
 # setup a role-key for your default feeds:
{{{
 $ cif_apikeys -u role_everyone_feed -a -g everyone -G everyone
}}}
 # install the apache2 config
  # [Beta3Apache2Debian Debian Based]
  # your config should look something like
{{{
<Location /api>
    SetHandler perl-script
    PerlSetVar Apache2RESTHandlerRootClass "CIF::WebAPI::Plugin"
    PerlSetVar Apache2RESTAPIBase "/api"
    PerlResponseHandler Apache2::REST
    PerlSetVar Apache2RESTWriterDefault 'json'
    PerlSetVar Apache2RESTAppAuth 'CIF::WebAPI::AppAuth'

    # feed defaults
    PerlSetVar CIFLookupLimitDefault 500
    PerlSetVar CIFDefaultFeedSeverity "high"

    # extra outputs
    PerlAddVar Apache2RESTWriterRegistry 'table'
    PerlAddVar Apache2RESTWriterRegistry 'CIF::WebAPI::Writer::table'
</Location>
}}}
 # add your "www-data" user (whoever apache is set to run under) to the group "cif" (/etc/group):
{{{
$ sudo adduser www-data cif
}}}
 # restart apache2
== Load Data ==
 # perform [Tools_cif_crontool cif_crontool's] "first run" to prime the database with it's initial intel (should take about 30min).
{{{
$ time /opt/cif/bin/cif_crontool -f -d && /opt/cif/bin/cif_crontool -d -p daily && /opt/cif/bin/cif_crontool -d -p hourly
}}}
 # after this starts, run your first batch of analytics (the first batch might take a few hours up through a few days depending on how much data you have, as long as you keep seeing uuid's fly across the screen.. you're good). If you have more cores/ram, you can increase -t/-m respectively. Sending cif_analytic a "KILL -INT" (CTRL+C) will spin down the process when it's finished the current batch so you can adjust accordingly. It'll pick back up where it left off.
  * a good setting for less than 8 cores and 8 gig of ram
{{{
$ time /opt/cif/bin/cif_analytic -d -t 16 -m 2500
}}}
  * a good setting for 8 cores and 16 gig of ram (this will crush your host for a while, esp your disk io, use htop to keep an eye that you're not swapping yourself to death)
{{{
$ time /opt/cif/bin/cif_analytic -d -t 32 -m 5000
}}}
 * when that's finished, run your first batch of feeds (could take anywhere from 20min to 2 hours depending on system, data load, etc)
{{{
$ time /opt/cif/bin/cif_feeds -d
}}}
== Finishing up ==
 # log into the cif user (sudo su - cif) and modify it's cron tab (crontab -e)
{{{
# set the path
PATH=/bin:/usr/local/bin:/opt/cif/bin

# run analytics
*/5 * * * * /opt/cif/bin/cif_analytic -t 16 -m 2500 &> /dev/null

# pull feed data
05     *       * * * /opt/cif/bin/cif_crontool -p hourly -T medium &> /dev/null
30     00      * * * /opt/cif/bin/cif_crontool -p daily -T low &> /dev/null

# update the feeds
45     *       * * * /opt/cif/bin/cif_feeds &> /dev/null
}}}
 # or if you want to setup logging (as an example)
{{{
# set the path
PATH=/bin:/usr/local/bin:/opt/cif/bin

# run analytics
*/2 * * * * /opt/cif/bin/cif_analytic -d -t 16 -m 2500 >> /home/cif/analytics.log 2>&1

# pull feed data
05     *       * * * /opt/cif/bin/cif_crontool -p hourly -T medium >> /home/cif/crontool_hourly.log 2>&1
30     00      * * * /opt/cif/bin/cif_crontool -p daily -T low >> /home/cif/crontool_daily.log 2>&1

# update the feeds
45     *       * * * /opt/cif/bin/cif_feeds >> /home/cif/feeds.log 2>&1
}}}